# ğŸ—ï¸ Kubernetes Cluster Architecture

## System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              INGRESS LAYER                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              NGINX Ingress Controller (Port 80 â†’ 3000)                 â”‚    â”‚
â”‚  â”‚                    Route: localhost:3000/*                              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                          â”‚
              â”‚                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚      APPLICATION LAYER (Worker Nodes)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚   REST API Pod    â”‚      â”‚   Webhook Pod       â”‚                             â”‚
â”‚  â”‚  (main.ts)        â”‚      â”‚  (main-stripe-      â”‚                             â”‚
â”‚  â”‚  Port: 3001       â”‚      â”‚   webhook.ts)       â”‚                             â”‚
â”‚  â”‚  Replicas: 2-10   â”‚      â”‚  Port: 3003         â”‚                             â”‚
â”‚  â”‚  HPA: âœ“           â”‚      â”‚  Replicas: 2-6      â”‚                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚             â”‚                         â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚   gRPC Pod        â”‚â—„â”€â”€â”€â”€â–ºâ”‚   Worker Pod        â”‚                             â”‚
â”‚  â”‚  (main-grpc.ts)   â”‚      â”‚  (main-worker.ts)   â”‚                             â”‚
â”‚  â”‚  Port: 50051      â”‚      â”‚  RabbitMQ Consumer  â”‚                             â”‚
â”‚  â”‚  Replicas: 2-8    â”‚      â”‚  Replicas: 3-10     â”‚                             â”‚
â”‚  â”‚  HPA: âœ“           â”‚      â”‚  HPA: âœ“             â”‚                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                          â”‚
              â”‚                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚     DATA/PERSISTENCE LAYER (StatefulSets)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚    MongoDB        â”‚      â”‚    RabbitMQ         â”‚                             â”‚
â”‚  â”‚  Port: 27017      â”‚      â”‚  Port: 5672, 15672  â”‚                             â”‚
â”‚  â”‚  PVC: 10Gi        â”‚      â”‚  PVC: 5Gi           â”‚                             â”‚
â”‚  â”‚  Replicas: 1      â”‚      â”‚  Replicas: 1        â”‚                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚  â”‚    Redis          â”‚      â”‚    ClickHouse       â”‚                             â”‚
â”‚  â”‚  Port: 6379       â”‚      â”‚  Port: 8123, 9000   â”‚                             â”‚
â”‚  â”‚  PVC: 5Gi         â”‚      â”‚  PVC: 10Gi          â”‚                             â”‚
â”‚  â”‚  Replicas: 1      â”‚      â”‚  Replicas: 1        â”‚                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Node Distribution

### Control Plane Node
- **Role**: Cluster management, scheduling, API server
- **Components**: 
  - kube-apiserver
  - etcd
  - kube-scheduler
  - kube-controller-manager

### Worker Node 1 - REST API
```yaml
Service: driveline-api (main.ts)
Port: 3001
Replicas: 2-10 (HPA enabled)
Resources:
  Request: 300m CPU, 512Mi RAM
  Limit: 1000m CPU, 1Gi RAM
Probes: Liveness, Readiness, Startup
```

### Worker Node 2 - gRPC Microservice
```yaml
Service: driveline-grpc (main-grpc.ts)
Port: 50051
Replicas: 2-8 (HPA enabled)
Resources:
  Request: 300m CPU, 512Mi RAM
  Limit: 1000m CPU, 1Gi RAM
Probes: TCP Socket checks
```

### Worker Node 3 - Webhook Service
```yaml
Service: driveline-webhook (main-stripe-webhook.ts)
Port: 3003
Replicas: 2-6 (HPA enabled)
Resources:
  Request: 200m CPU, 256Mi RAM
  Limit: 500m CPU, 512Mi RAM
Probes: HTTP health checks
```

### Worker Node 4 - RabbitMQ Worker
```yaml
Service: driveline-worker (main-worker.ts)
Queue: worker_queue_v1
Replicas: 3-10 (HPA enabled)
Resources:
  Request: 300m CPU, 512Mi RAM
  Limit: 1000m CPU, 1Gi RAM
Probes: Process health checks
```

## Traffic Flow

### External Request Flow
```
User Request â†’ localhost:3000
    â†“
NGINX Ingress Controller (Port Forwarding)
    â†“
Ingress Rules (Path-based routing)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ /api/*     â†’ API Service (3001) â”‚
â”‚ /webhook/* â†’ Webhook Service    â”‚
â”‚ /*         â†’ API Service        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Backend Services (Load Balanced)
    â†“
Database/Cache Services
```

### Internal Service Communication
```
REST API â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ gRPC Service
   â”‚                         â”‚
   â”œâ”€â†’ MongoDB â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”œâ”€â†’ Redis   â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â””â”€â†’ ClickHouse

Webhook Service â”€â”€â†’ RabbitMQ â”€â”€â†’ Worker Service
                                      â”‚
                                      â”œâ”€â†’ MongoDB
                                      â”œâ”€â†’ gRPC
                                      â””â”€â†’ External APIs
```

## Network Policies

### API Pod Communication
```yaml
Ingress:
  - From: NGINX Ingress (Port 3001)
Egress:
  - To: MongoDB (27017)
  - To: Redis (6379)
  - To: RabbitMQ (5672)
  - To: ClickHouse (8123)
  - To: gRPC Service (50051)
  - To: Internet (443, 80)
```

### gRPC Pod Communication
```yaml
Ingress:
  - From: API Pods (50051)
  - From: Worker Pods (50051)
Egress:
  - To: MongoDB (27017)
  - To: Redis (6379)
  - To: Internet (443, 80)
```

### Webhook Pod Communication
```yaml
Ingress:
  - From: NGINX Ingress (Port 3003)
Egress:
  - To: MongoDB (27017)
  - To: RabbitMQ (5672)
  - To: Internet (443, 80)
```

### Worker Pod Communication
```yaml
Egress Only:
  - To: MongoDB (27017)
  - To: RabbitMQ (5672)
  - To: gRPC Service (50051)
  - To: Internet (443, 80)
```

## High Availability Features

### 1. Pod Disruption Budgets (PDB)
- **API**: Minimum 1 pod available during disruptions
- **gRPC**: Minimum 1 pod available
- **Webhook**: Minimum 1 pod available
- **Worker**: Minimum 2 pods available

### 2. Horizontal Pod Autoscaling (HPA)
| Service | Min | Max | CPU Target | Memory Target |
|---------|-----|-----|------------|---------------|
| API | 2 | 10 | 70% | 80% |
| gRPC | 2 | 8 | 70% | 80% |
| Webhook | 2 | 6 | 70% | 80% |
| Worker | 3 | 10 | 70% | 80% |

### 3. Pod Anti-Affinity
- Pods of the same component prefer different nodes
- Improves fault tolerance
- Prevents single point of failure

### 4. Rolling Updates
```yaml
Strategy:
  Type: RollingUpdate
  MaxSurge: 1        # Create 1 extra pod during update
  MaxUnavailable: 0  # Keep all pods running during update
```

### 5. Health Checks
- **Liveness Probe**: Restart pod if unhealthy
- **Readiness Probe**: Remove from service if not ready
- **Startup Probe**: Allow long startup times

## Resource Allocation

### Total Cluster Resources (Minimum)
```yaml
CPUs: 4 cores
Memory: 8 GB
Storage: 35 GB (PersistentVolumes)
  - MongoDB: 10 GB
  - Redis: 5 GB
  - RabbitMQ: 5 GB
  - ClickHouse: 10 GB
  - System: 5 GB
```

### Per-Service Limits
```yaml
API Service (per pod):
  CPU: 200m request, 1000m limit
  Memory: 512Mi request, 1Gi limit

gRPC Service (per pod):
  CPU: 300m request, 1000m limit
  Memory: 512Mi request, 1Gi limit

Webhook Service (per pod):
  CPU: 200m request, 500m limit
  Memory: 256Mi request, 512Mi limit

Worker Service (per pod):
  CPU: 300m request, 1000m limit
  Memory: 512Mi request, 1Gi limit
```

## Monitoring Endpoints

### Application Metrics
```
API Health:     http://localhost:3000/api/v1/health
Webhook Health: http://localhost:3000/webhook/v1/health
RabbitMQ UI:    http://localhost:30672 (guest/guest)
```

### Kubernetes Metrics
```bash
# Pod metrics
kubectl top pods -n driveline

# Node metrics
kubectl top nodes

# HPA status
kubectl get hpa -n driveline

# Resource usage
kubectl describe nodes
```

## Security Layers

1. **Network Policies**: Restrict pod-to-pod communication
2. **RBAC**: Role-based access control (namespace isolation)
3. **Secrets**: Encrypted sensitive data (base64 encoded)
4. **Resource Quotas**: Prevent resource exhaustion
5. **Pod Security**: Non-root containers, read-only filesystems

## Deployment Strategy

### Zero-Downtime Updates
1. Build new Docker image
2. Update image tag in deployment
3. Rolling update begins:
   - Create new pod
   - Wait for readiness probe
   - Remove old pod
   - Repeat for all replicas

### Rollback Capability
```bash
# View rollout history
kubectl rollout history deployment driveline-api -n driveline

# Rollback to previous version
kubectl rollout undo deployment driveline-api -n driveline

# Rollback to specific revision
kubectl rollout undo deployment driveline-api -n driveline --to-revision=2
```

---

## Best Practices Implemented

âœ… **Separation of Concerns**: Each service runs in dedicated pods
âœ… **Stateless Applications**: Application logic separate from data
âœ… **Immutable Infrastructure**: Containers are immutable
âœ… **Configuration Management**: ConfigMaps and Secrets
âœ… **Health Monitoring**: Comprehensive probe configuration
âœ… **Resource Management**: CPU and memory limits
âœ… **Auto-scaling**: HPA for dynamic load handling
âœ… **High Availability**: Multiple replicas with anti-affinity
âœ… **Network Security**: Network policies for isolation
âœ… **Persistent Storage**: StatefulSets for databases
âœ… **Graceful Shutdown**: Termination grace periods
âœ… **Rolling Updates**: Zero-downtime deployments

---

**This architecture follows Kubernetes best practices and is production-ready with proper monitoring, scaling, and security configurations.**
